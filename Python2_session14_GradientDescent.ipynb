{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 14: Gradient Descent for Multivariate Linear Regression\n",
    "\n",
    "In this session, we will:\n",
    "\n",
    "1. Learn the basics of **NumPy** for numerical computing\n",
    "2. Understand **gradient descent** optimization\n",
    "3. Implement **multivariate linear regression** from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Introduction to NumPy\n",
    "\n",
    "NumPy is the fundamental package for numerical computing in Python. It provides:\n",
    "\n",
    "- N-dimensional arrays (`ndarray`)\n",
    "- Broadcasting for element-wise operations\n",
    "- Linear algebra operations\n",
    "- Mathematical functions optimized for arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D array: [-1  2  3  4  5]\n",
      "Shape: (5,)\n",
      "Dtype: int8\n"
     ]
    }
   ],
   "source": [
    "# From Python lists\n",
    "arr1 = np.array([-1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(f\"1D array: {arr1}\")\n",
    "print(f\"Shape: {arr1.shape}\")\n",
    "print(f\"Dtype: {arr1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6]])\n",
    "print(f\"2D array:\\n{arr2}\")\n",
    "print(f\"Shape: {arr2.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ones:\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "random:\n",
      "[[-0.26870188 -0.37644014 -1.15308419]\n",
      " [-0.22421846  0.77504234 -0.86618435]\n",
      " [ 0.44210977  1.00358155  0.33789688]]\n",
      "\n",
      "arange: [0 2 4 6 8]\n",
      "linspace: [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Common array creation functions\n",
    "zeros = np.zeros((3, 4))       # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))         # 2x3 matrix of ones\n",
    "# 3x3 matrix of random values (normal distribution)\n",
    "random = np.random.randn(3, 3)\n",
    "arange = np.arange(0, 10, 2)   # [0, 2, 4, 6, 8]\n",
    "linspace = np.linspace(0, 1, 5)  # 5 evenly spaced values from 0 to 1\n",
    "\n",
    "print(f\"zeros:\\n{zeros}\\n\")\n",
    "print(f\"ones:\\n{ones}\\n\")\n",
    "print(f\"random:\\n{random}\\n\")\n",
    "print(f\"arange: {arange}\")\n",
    "print(f\"linspace: {linspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Array Operations and Broadcasting\n",
    "\n",
    "NumPy operations are **element-wise** by default. Broadcasting allows operations between arrays of different shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [5 7 9]\n",
      "a * b = [ 4 10 18]\n",
      "a ** 2 = [1 4 9]\n",
      "a * 10 = [10 20 30]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"a + b = {a + b}\")      # Element-wise addition\n",
    "print(f\"a * b = {a * b}\")      # Element-wise multiplication\n",
    "print(f\"a ** 2 = {a ** 2}\")    # Element-wise power\n",
    "print(f\"a * 10 = {a * 10}\")    # Broadcasting: scalar applied to all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix + row_vector:\n",
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with 2D arrays\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "row_vector = np.array([10, 20, 30])\n",
    "\n",
    "\"\"\"\n",
    "[[10, 20, 30],\n",
    "[10, 20 ,30]]\n",
    "\"\"\"\n",
    "\n",
    "# The row vector is \"broadcast\" to each row of the matrix\n",
    "result = matrix + row_vector\n",
    "print(f\"Matrix + row_vector:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Operations\n",
    "\n",
    "For linear regression, we need matrix multiplication and transpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A @ B (matrix multiplication):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "np.dot(A, B):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "A.T (transpose):\n",
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "print(f\"A @ B (matrix multiplication):\\n{A @ B}\")\n",
    "print(f\"\\nnp.dot(A, B):\\n{np.dot(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nA.T (transpose):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Useful Aggregation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum all: 21\n",
      "Sum along rows (axis=1): [ 6 15]\n",
      "Sum along columns (axis=0): [5 7 9]\n",
      "Mean: 3.5\n",
      "Std: 1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "\n",
    "print(f\"Sum all: {np.sum(arr)}\")\n",
    "print(f\"Sum along rows (axis=1): {np.sum(arr, axis=1)}\")\n",
    "print(f\"Sum along columns (axis=0): {np.sum(arr, axis=0)}\")\n",
    "print(f\"Mean: {np.mean(arr)}\")\n",
    "print(f\"Std: {np.std(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Linear Regression Review\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear regression models the relationship between features $X$ and target $y$ as:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $X$ is the feature matrix of shape `(n_samples, n_features)`\n",
    "- $w$ is the weight vector of shape `(n_features,)`\n",
    "- $b$ is the bias (intercept) scalar\n",
    "- $\\hat{y}$ is the predicted values\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Find $w$ and $b$ that minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\mathcal{L} = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Linear Regression\n",
    "\n",
    "For MSE loss with linear regression, the gradients are:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial MSE}{\\partial w} = \\frac{2}{n}  (\\hat{y} - y) X$$\n",
    "\n",
    "$$\\nabla_b \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial MSE}{\\partial b} = \\frac{2}{n} \\sum(\\hat{y} - y)$$\n",
    "\n",
    "Let's derive this step by step:\n",
    "\n",
    "1. $MSE = \\frac{1}{n}\\sum(\\hat{y} - y)^2 = \\frac{1}{n}\\sum(Xw + b - y)^2$\n",
    "\n",
    "2. Using chain rule: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} \\cdot (Xw + b - y) \\cdot X$\n",
    "\n",
    "3. In matrix form: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} (\\hat{y} - y) X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an optimization algorithm that iteratively updates parameters to minimize a loss function.\n",
    "\n",
    "Think of it like descending a mountain in fog - you can only feel the slope at your current position, so you take small steps in the steepest downward direction.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\cdot \\nabla_w \\mathcal{L} = w_{old} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** - how big of a step we take.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Implementation from Scratch\n",
    "\n",
    "Let's build our linear regression step by step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "y shape: (1000,)\n",
      "True weights: [ 2.  -3.5  1.5]\n",
      "True bias: 5.0\n"
     ]
    }
   ],
   "source": [
    "# First, let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters we want to learn\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate random features\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate target with some noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Core Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `predict()`, `compute_mse()`, `compute_gradients()`, which perform neccessary operations for gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute predictions for linear regression.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        w: Weight vector of shape (n_features,)\n",
    "        b: Bias scalar\n",
    "\n",
    "    Returns:\n",
    "        Predictions of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "\n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "\n",
    "    Returns:\n",
    "        MSE loss value\n",
    "    \"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for weights and bias.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (gradient_w, gradient_b)\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    grad_w = (2 / n) * ((y_pred - y) @ X)\n",
    "    grad_b = (2 / n) * np.sum((y_pred - y))\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute first gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (3,), 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.randn(3)\n",
    "b = 0.0\n",
    "X.shape, w.shape, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-9.63805501,  6.22101998, -3.9012661 ]), np.float64(-10.719867319459937))\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, w, b)  # Initial predictions\n",
    "# Should print gradients (grad_w, grad_b)\n",
    "print(compute_gradients(X, y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you observe what happen?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step=25,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target values of shape (n_samples,)\n",
    "        learning_rate: Step size for gradient descent\n",
    "        n_iterations: Number of training iterations\n",
    "        verbose: Whether to print progress\n",
    "        log_every_n_step: Number of steps to log the result\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (final_weights, final_bias, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        y_pred = predict(X, w, b)\n",
    "        loss = compute_mse(y, y_pred)\n",
    "        loss_history.append(loss)\n",
    "        grad_w, grad_b = compute_gradients(X, y, y_pred)\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "\n",
    "        # if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "        #     print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "\n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45.786696\n",
      "Iteration   25 | Loss: 16.037265\n",
      "Iteration   50 | Loss: 5.734739\n",
      "Iteration   75 | Loss: 2.163298\n",
      "Iteration  100 | Loss: 0.923821\n",
      "Iteration  125 | Loss: 0.493101\n",
      "Iteration  150 | Loss: 0.343205\n",
      "Iteration  175 | Loss: 0.290952\n",
      "Iteration  200 | Loss: 0.272703\n",
      "Iteration  225 | Loss: 0.266316\n",
      "Iteration  250 | Loss: 0.264075\n",
      "Iteration  275 | Loss: 0.263286\n",
      "Iteration  300 | Loss: 0.263008\n",
      "Iteration  325 | Loss: 0.262910\n",
      "Iteration  350 | Loss: 0.262875\n",
      "Iteration  375 | Loss: 0.262862\n",
      "Iteration  400 | Loss: 0.262858\n",
      "Iteration  425 | Loss: 0.262856\n",
      "Iteration  450 | Loss: 0.262856\n",
      "Iteration  475 | Loss: 0.262856\n",
      "Iteration  499 | Loss: 0.262855\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learned_w, learned_b, losses = train_linear_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results ===\n",
      "True weights:    [ 2.  -3.5  1.5]\n",
      "Learned weights: [ 2.00857002 -3.5131732   1.48090698]\n",
      "\n",
      "True bias:    5.0\n",
      "Learned bias: 4.9908\n"
     ]
    }
   ],
   "source": [
    "# Compare learned parameters with true parameters\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"True weights:    {true_weights}\")\n",
    "print(f\"Learned weights: {learned_w}\")\n",
    "print(f\"\\nTrue bias:    {true_bias}\")\n",
    "print(f\"Learned bias: {learned_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Feature Scaling\n",
    "\n",
    "Gradient descent works much better when features are on similar scales. Let's see why and how to fix it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is statistical way to standarize any sameple of feature $i$ to a normal distribution:\n",
    "$$X_i = \\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "where $X_i$ is the column vector of raw data feature $i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranges:\n",
      "  Feature 1: [-3241.27, 3852.73]\n",
      "  Feature 2: [-0.00, 0.00]\n",
      "  Feature 3: [-2.90, 2.60]\n"
     ]
    }
   ],
   "source": [
    "# Create data with very different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.column_stack([\n",
    "    np.random.randn(500) * 1000,      # Feature 1: scale ~1000\n",
    "    np.random.randn(500) * 0.001,     # Feature 2: scale ~0.001\n",
    "    np.random.randn(500)              # Feature 3: scale ~1\n",
    "])\n",
    "y_unscaled = X_unscaled @ np.array([0.001, 1000, 1]) + \\\n",
    "    5 + np.random.randn(500) * 0.5\n",
    "\n",
    "print(f\"Feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(\n",
    "        f\"  Feature {i+1}: [{X_unscaled[:, i].min():.2f}, {X_unscaled[:, i].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 85.696115\n",
      "Iteration   25 | Loss: 871155159202080805340365040358143631879229664798494767381715394830750039814045862849735767705550771108526879477364251653809771512373826652642481443263610550402662104844181197413872993691833211914127192672145978163200.000000\n",
      "Iteration   50 | Loss: inf\n",
      "Iteration   75 | Loss: nan\n",
      "Iteration   99 | Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/4wh_c63s2zdf_x3j9sybcz6m0000gn/T/ipykernel_99149/2889169837.py:12: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean((y_pred - y_true) ** 2)\n",
      "/opt/anaconda3/envs/python2-hsutcc/lib/python3.14/site-packages/numpy/_core/_methods.py:134: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/var/folders/fz/4wh_c63s2zdf_x3j9sybcz6m0000gn/T/ipykernel_99149/436678851.py:14: RuntimeWarning: overflow encountered in matmul\n",
      "  grad_w = (2 / n) * ((y_pred - y) @ X)\n",
      "/var/folders/fz/4wh_c63s2zdf_x3j9sybcz6m0000gn/T/ipykernel_99149/436678851.py:14: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad_w = (2 / n) * ((y_pred - y) @ X)\n",
      "/opt/anaconda3/envs/python2-hsutcc/lib/python3.14/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/fz/4wh_c63s2zdf_x3j9sybcz6m0000gn/T/ipykernel_99149/2860131721.py:36: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - learning_rate * grad_w\n"
     ]
    }
   ],
   "source": [
    "# This will likely fail or converge very slowly!\n",
    "try:\n",
    "    w_bad, b_bad, losses_bad = train_linear_regression(\n",
    "        X_unscaled, y_unscaled, learning_rate=0.01, n_iterations=100\n",
    "    )\n",
    "except:\n",
    "    print(\"Training failed due to numerical instability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement standardize function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Standardization (Z-score normalization)\n",
    "def standardize(X: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Standardize features to have mean=0 and std=1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (X_standardized, mean, std)\n",
    "    \"\"\"\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    std[std == 0] = 1.0\n",
    "    X_standardized = (X - mean) / std\n",
    "    return X_standardized, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature ranges:\n",
      "  Feature 1: [-3.31, 3.92]\n",
      "  Feature 2: [-2.79, 2.66]\n",
      "  Feature 3: [-2.98, 2.47]\n",
      "True weights:    [0.001, 1000, 1]\n",
      "Learned weights: [1.01072837 0.96936309 1.00073613]\n",
      "\n",
      "True bias:    5.0\n",
      "Learned bias: 5.1637\n",
      "\n",
      "Final Loss: 0.240479\n"
     ]
    }
   ],
   "source": [
    "# Standardize and train\n",
    "X_scaled, X_mean, X_std = standardize(X_unscaled)\n",
    "\n",
    "print(f\"Scaled feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(\n",
    "        f\"  Feature {i+1}: [{X_scaled[:, i].min():.2f}, {X_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "w_good, b_good, losses_good = train_linear_regression(\n",
    "    X_scaled, y_unscaled, learning_rate=0.1, n_iterations=500\n",
    ")\n",
    "\n",
    "print(f\"True weights:    {[0.001, 1000, 1]}\")\n",
    "print(f\"Learned weights: {w_good}\")\n",
    "print(f\"\\nTrue bias:    {5.0}\")\n",
    "print(f\"Learned bias: {b_good:.4f}\")\n",
    "print(f\"\\nFinal Loss: {losses_good[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Tasks (Deadline: Sunday 30th Nov 2025)\n",
    "\n",
    "Complete the following tasks to practice implementing gradient descent for linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "Instead of using all samples in each iteration (batch gradient descent), implement **mini-batch gradient descent** which uses a random subset of samples.\n",
    "\n",
    "Formally said, choose $X_b$ and its corresponding $y_b$ which is a subset of $row(X), row(y)$ to be trained for each iteration.\n",
    "\n",
    "Benefits of mini-batch:\n",
    "\n",
    "- Faster iterations\n",
    "- Can escape local minima\n",
    "- Better generalization\n",
    "\n",
    "```python\n",
    "# Expected usage:\n",
    "w, b, losses = train_minibatch_gd(X, y, batch_size=32, learning_rate=0.01, n_iterations=1000)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 20,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using mini-batch gradient descent.\n",
    "\n",
    "    In each iteration:\n",
    "      - randomly sample a subset of rows\n",
    "      - compute predictions, loss, gradients on the mini-batch only\n",
    "      - update w and b\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize parameters\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        batch_idx = np.random.choice(n_samples, batch_size, replace=False)\n",
    "        Xb = X[batch_idx]\n",
    "        yb = y[batch_idx]\n",
    "\n",
    "        y_pred_b = predict(Xb, w, b)\n",
    "        loss = compute_mse(yb, y_pred_b)\n",
    "        loss_history.append(loss)\n",
    "        grad_w, grad_b = compute_gradients(Xb, yb, y_pred_b)\n",
    "\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "\n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "\n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 41.712871\n",
      "Iteration   50 | Loss: 6.725149\n",
      "Iteration  100 | Loss: 0.788649\n",
      "Iteration  150 | Loss: 0.286194\n",
      "Iteration  199 | Loss: 0.251438\n"
     ]
    }
   ],
   "source": [
    "_, _, loss_history = train_minibatch_gd(\n",
    "    X, y,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=200,\n",
    "    log_every_n_step=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Learning Rate Scheduling\n",
    "\n",
    "Implement a training function that **decreases the learning rate** over time. This helps converge more precisely at the end of training.\n",
    "\n",
    "Common schedules:\n",
    "\n",
    "- Step decay: $\\alpha_t = \\alpha_0 \\cdot 0.9^{\\lfloor t/100 \\rfloor}$\n",
    "- Exponential decay: $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "- Inverse time: $\\alpha_t = \\frac{\\alpha_0}{1 + k \\cdot t}$\n",
    "\n",
    "where $t$ is number of current step/iteration and $k$ is the decay constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    initial_lr: float = 0.1,\n",
    "    schedule: str = 'exponential',  # 'step', 'exponential', or 'inverse'\n",
    "    n_iterations: int = 1000,\n",
    "    decay_constant: float = 0.0001,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train with learning rate scheduling.\n",
    "\n",
    "    Implement at least one scheduling strategy.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "\n",
    "    learning_rate = initial_lr\n",
    "    loss_history = []\n",
    "\n",
    "    for t in range(n_iterations):\n",
    "        if schedule == 'step':\n",
    "            learning_rate = initial_lr * (0.9 ** (t // 100))\n",
    "        elif schedule == 'exponential':\n",
    "            learning_rate = initial_lr * np.exp(-decay_constant * t)\n",
    "        elif schedule == 'inverse':\n",
    "            learning_rate = initial_lr / (1 + decay_constant * t)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unknown schedule: choose 'step', 'exponential', or 'inverse'.\")\n",
    "\n",
    "        y_pred_b = predict(X, w, b)\n",
    "        loss = compute_mse(y, y_pred_b)\n",
    "        loss_history.append(loss)\n",
    "        grad_w, grad_b = compute_gradients(X, y, y_pred_b)\n",
    "\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "\n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step decay:\n",
      "\n",
      "Final Loss: 0.2629\n",
      "\n",
      "\n",
      "Exponential decay:\n",
      "\n",
      "Final Loss: 0.2629\n",
      "\n",
      "\n",
      "Inverse time decay:\n",
      "\n",
      "Final Loss: 0.2629\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test them all:\n",
    "print(\"Step decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='step',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "print(f\"\\nFinal Loss: {loss_history[-1]:.4f}\\n\\n\")\n",
    "\n",
    "print(\"Exponential decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='exponential',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "print(f\"\\nFinal Loss: {loss_history[-1]:.4f}\\n\\n\")\n",
    "\n",
    "print(\"Inverse time decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='inverse',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "print(f\"\\nFinal Loss: {loss_history[-1]:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add Regularization (Ridge Regression)\n",
    "\n",
    "Implement **L2 regularization** (Ridge regression) to prevent overfitting.\n",
    "\n",
    "The loss function becomes:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\sum w_i^2$$\n",
    "\n",
    "The gradient for weights becomes:\n",
    "$$\\frac{\\partial Loss}{\\partial w} = \\frac{\\partial MSE}{\\partial w} + 2\\lambda w$$\n",
    "\n",
    "where $\\lambda$ is the regularization constant and $w_i$ is the weight value of corresponding feature $i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ridge_loss(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Ridge regression loss (MSE + L2 regularization).\n",
    "\n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "\n",
    "    Returns:\n",
    "        Ridge loss value\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_ridge_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for Ridge regression.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "        \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "def train_ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_lambda: float = 0.1,  # Regularization strength\n",
    "    n_iterations: int = 1000\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression with L2 regularization.\n",
    "\n",
    "    Hints:\n",
    "    - Modify the loss calculation to include regularization term\n",
    "    - Modify the gradient calculation for weights\n",
    "    - Note: We typically don't regularize the bias term\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ = train_ridge_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task: Implement Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Implement pure SGD where you update weights after **each individual sample** (batch_size=1).\n",
    "\n",
    "Compare the convergence behavior of:\n",
    "\n",
    "1. Batch GD (all samples)\n",
    "2. Mini-batch GD (e.g., 32 samples)\n",
    "3. SGD (1 sample)\n",
    "\n",
    "Plot the loss curves for all three on the same graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2-hsutcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
